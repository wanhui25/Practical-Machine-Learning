---
title: "Prediction Assignment"
author: "Hui Wan"
date: "January 31, 2016"
---

<font size="3"><b>Introduction:</b></font><br>
The goal of this project is to use data from accelerometers on the belt, forearm, arm and dumbell of 6 participants to predict the manner in which they did the exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The following questions will be addressed in this report.

1. How you built your model
2. How you used cross validation
3. What you think the expected out of sample error is
4. Why you made the choices you did. 

At end, I will use the most accurate prediction model to predict on 20 different test cases. 

<font size="3"><b>Order of Important Steps:</b></font><br>
As instructed in week 1 lecture, this report will be presented in the following order

Question - Data - Features - Algorithm

Preload the package and see the seed (123) of reproduceable results.
```{r, warning=FALSE, message=FALSE}
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)
```

<font size="3"><b>Question</b></font><br>
In this project, six participants perform in a dumbell lifting exercise in five differernt ways.<br>
Class A - according to the specification<br>
Class B - throwing the elbows to the front<br>
Class C - lifting the dumbbell only halfway<br>
Class D - lowering the dumbbell only halfway<br>
Class E - throwing the hips to the front<br>

The question of this project is whether we can accurately predict activity class in testing set based on the prediction model established from given training dataset. 

<font size="3"><b>Data</b></font><br>
I saved both training and testing csv into local project folder and load them into training and testing set in RStudio
```{r, warning=FALSE}
setwd("C:/Users/HUI/desktop/Coursera/Practical Machine Learning/Project")
training <-read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"), header=TRUE)
testing <- read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"), header=TRUE)
training_colnames <- colnames(training)

dim(training)
dim(testing)
summary(training$classe)
```

<font size="3"><b>Feature</b></font><br>
After briefly inspecting the training set, I found many NA data and first seven columns that are not helpful for developing prediction model. Therefore, the following code is to clearn the data. 
```{r}
NotNA <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

colcnts <- NotNA(training)
drops <- c()
for (cnt in 1:length(colcnts)) {
    if (colcnts[cnt] < nrow(training)) {
        drops <- c(drops, training_colnames[cnt])
    }
}

# Drop NA data and the first 7 columns
training <- training[,!(names(training) %in% drops)]
training <- training[,8:length(colnames(training))]

testing <- testing[,!(names(testing) %in% drops)]
testing <- testing[,8:length(colnames(testing))]

# Show remaining variables.
colnames(training)
```

<font size="3"><b>Algorithm</b></font><br>
My plan of algorithm is to use both Classification Tree and Random Forest either with or without preprocessing and cross validation to find a prediction model with best accuracy, and use this model to predict on the testing set. 

Sine the training set is large (19622 entries) but testing set is small (20 entries). Then I decide to divide the training set into 4 equal dataset for the following 4 scenario, each dataset will be split into a training and testing set (60%:40%) for model developing and testing purpose. 

1. Classification Tree without preprocessing and cross validation
2. Classification Tree with preprocessing and cross validation
3. Random Forest without preprocessing and cross validation
4. Random Forest with preprocessing and cross validation

```{r}
# Divide entire training set into 4 smaller data set
set.seed(123)
intrain<- createDataPartition(y=training$classe, p=0.25, list=FALSE)
training1 <- training[intrain,]
remainder <- training[-intrain,]
set.seed(123)
intrain <- createDataPartition(y=remainder$classe, p=0.33, list=FALSE)
training2 <- remainder[intrain,]
remainder <- remainder[-intrain,]
set.seed(123)
intrain <- createDataPartition(y=remainder$classe, p=0.5, list=FALSE)
training3 <- remainder[intrain,]
training4 <- remainder[-intrain,]

# Divide each of these 4 sets into training (60%) and test (40%) sets.
set.seed(123)
inTrain <- createDataPartition(y=training1$classe, p=0.6, list=FALSE)
training1 <- training1[inTrain,]
testing1 <- training1[-inTrain,]
set.seed(123)
inTrain <- createDataPartition(y=training2$classe, p=0.6, list=FALSE)
training2 <- training2[inTrain,]
testing2 <- training2[-inTrain,]
set.seed(123)
inTrain <- createDataPartition(y=training3$classe, p=0.6, list=FALSE)
training3 <- training3[inTrain,]
testing3 <- training3[-inTrain,]
set.seed(123)
inTrain <- createDataPartition(y=training4$classe, p=0.6, list=FALSE)
training4 <- training4[inTrain,]
testing4 <- training4[-inTrain,]
```

<font size="2"><b><I>Classification Tree</I></b></font><br>
1. Classification Tree without preprocessing and cross validation
```{r}
# Classification Tree on training set 1 of 4 with no extra features.
set.seed(123)
model1 <- train(classe ~ ., method="rpart", data = training1)

# Predict on testing set 1 of 4 with no extra features.
predictions <- predict(model1, newdata=testing1)
confusionMatrix(predictions, testing1$classe)
```
The accuracy of this model is only 48.95% and hope it will be significantly improved by introduction of preprocessing and cross validation.

Out of Sample Error: 1 - 0.4895 = 51.05%

2. Classification Tree with preprocessing and cross validation
```{r}
# Classification Tree on training set 2 of 4 with preprocessing and cross validation
set.seed(123)
model2 <- train(classe ~ ., method="rpart",  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = training2)

# predict on testing set 2 of 4 with both preprocessing and cross validation.
predictions <- predict(model2, newdata=testing2)
confusionMatrix(predictions, testing2$classe)
```
The model of Classification Tree with preprocessing and cross validation only give 43.78% accuracy, even worse than Classification Tree with no extra feature. 

Out of Sample Error: 1 - 0.4378 = 56.22%

<font size="2"><b><I>Random Forest</I></b></font><br>
3. Random Forest without preprocessing and cross validation
```{r}
# Random Forest on training set 3 of 4 with no extra features.
set.seed(123)
model3 <- train(classe ~ ., method="rf",preProcess=c("center", "scale"), data=training3)

# Predict on testing set 3 of 4 with no extra features.
predictions <- predict(model3, newdata=testing3)
confusionMatrix(predictions, testing3$classe)
```
Random Forest with no extra feature delivers 100% accuracy on the 3rd testing set. This model will be used to predict on the testing set. 

Out of Sample Error: 1 - 1 = 0%

```{r}
# Use this model against the testing set
predict(model3, newdata=testing)
```

4. Random Forest with preprocessing and cross validation
```{r}
# Random Forest on training set 4 of 4 with both preprocessing and cross validation.
set.seed(123)
model4 <- train(classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=training4)

# predict on testing set 4 of 4 with both preprocessing and cross validation.
predictions <- predict(model4, newdata=testing4)
confusionMatrix(predictions, testing4$classe)
```
Random Forest with preprocessing and cross validation predication also gives the best accuracy 100%.Therefore I will use model 3 and 4 to predict on the testing set given in the project.

Out of Sample Error: 1 - 100% = 0%

```{r}
predict(model4, newdata=testing)
```

<font size="3"><b>Conclusion</b></font><br>
Even both model 3 and moedel 4 developed from Random Forest method giving 100% accuacy in testing set 3 and 4. But they still produce a slight difference result on the final 20 different test cases as shown below. Therefore, we can conclude that a model built with low or zero out of sample error can still prone to error when it is used to predicts in another brand new data set.   

Model 3 - [1] B A B A A E D B A A B C B A E E A B B B
Model 4 - [1] C A B A A E D D A A B C B A E E A B B B
